{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600512906683",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import foolbox as fb\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTClassifer(pl.LightningModule):\n",
    "    def __init__(self, regular_train=True, adv_train=False):\n",
    "        super(MNISTClassifer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=28*28*64, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=10)\n",
    "        self.attack_type = fb.attacks.L2FastGradientAttack()\n",
    "        self.adv_train = adv_train\n",
    "        self.regular_train = regular_train\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_data = MNIST(root=\"data/\", train=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
    "\n",
    "        self.val_data = MNIST(root=\"data/\", train=False, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor()]))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_data, batch_size=128, shuffle=True, num_workers=12)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_data, batch_size=128, shuffle=False, num_workers=12)\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \n",
    "        return_dict = {}\n",
    "\n",
    "        data, label = train_batch\n",
    "\n",
    "        if self.regular_train:\n",
    "            out = self.forward(data)\n",
    "            loss = F.nll_loss(out, label)\n",
    "            prediction = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct =  prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            return_dict['loss'] = loss \n",
    "            return_dict['correct'] = correct \n",
    "\n",
    "        if self.adv_train:\n",
    "            self.eval()\n",
    "            adv_model = fb.PyTorchModel(self, bounds=(0,1))\n",
    "            adv_data, _, success = self.attack_type(adv_model, data, label, epsilons=[0.01])\n",
    "            print(adv_data)\n",
    "            successful_attack_no = torch.sum(success.long())\n",
    "            self.zero_grad()\n",
    "            out = self.forward(adv_data)\n",
    "            adv_loss = F.nll_loss(out, label)\n",
    "            return_dict['successful_attack'] = successful_attack_no\n",
    "            return_dict['adv_loss'] = adv_loss\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        #end of an epoch\n",
    "        #calculate the average loss of this epoch}\n",
    "        return_dict = {}\n",
    "        logs = {}\n",
    "        if self.regular_train:\n",
    "            avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "            accuracy = 100 * (sum([x['correct'] for x in outputs]) / float(len(self.train_data)))\n",
    "            logs['train_loss'] = avg_loss\n",
    "            logs['train_accuracy'] = accuracy\n",
    "            return_dict['loss'] = avg_loss\n",
    "            \n",
    "        if self.adv_train:\n",
    "            adv_avg_loss = torch.stack([x['adv_loss'] for x in outputs]).mean()\n",
    "            adv_accuracy = (sum([x['successful_attack'] for x in outputs]) / float(len(self.train_data)))\n",
    "            logs['adv_loss'] = adv_avg_loss\n",
    "            logs['robust_accuracy'] = 100. * (1. - adv_accuracy)\n",
    "            return_dict['adv_loss'] = adv_loss\n",
    "\n",
    "                \n",
    "        return {'loss' : avg_loss, 'log' : logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, validation_batch, batch_idx):\n",
    "        if batch_idx == 0:\n",
    "            torch.cuda.memory_summary()\n",
    "        \n",
    "        data, label = validation_batch\n",
    "        #standard inference\n",
    "        out = self.forward(data)\n",
    "        loss = F.nll_loss(out, label)\n",
    "        prediction = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "        correct =  prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "        return {'loss' : loss, 'correct' : correct}\n",
    "\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #end of an epoch\n",
    "        #calculate the average loss of this epoch\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        accuracy = 100. * (sum([x['correct'] for x in outputs]) / float(len(self.val_data)))\n",
    "\n",
    "        logs = {'validation_loss': avg_loss, 'validation_accuracy': accuracy}\n",
    "        return {'loss' : avg_loss, 'log' : logs}\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "\n",
    "    def adversarial_validation(self):\n",
    "        val_dataloader = self.val_dataloader()\n",
    "        self.eval()\n",
    "        adv_model = fb.PyTorchModel(self, bounds=(0,1))\n",
    "        successful_attack_sum = 0\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            for batch_id, (data, label) in enumerate(val_dataloader):\n",
    "                data, label = data.cuda(), label.cuda()\n",
    "                _, _, success = self.attack_type(adv_model, data, label, epsilons=[0.01])\n",
    "                successful_attack_no = torch.sum(success.long())\n",
    "\n",
    "                successful_attack_sum += successful_attack_no\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "        print(\"Successful attack:{} Attack count:{} Percentage:{}\".format(successful_attack_sum, len(self.val_data), float(successful_attack_sum) /len(self.val_data)))\n",
    "        robust_accuracy = 100. * (1-(float(successful_attack_sum) /len(self.val_data))) \n",
    "        return robust_accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTClassifer(regular_train=True, adv_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger('robust_gan_logs', name=\"mnist_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n"
    },
    {
     "output_type": "error",
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0]\n                But your machine only has: []\n            ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-187c0e916634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_dev_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, early_stop_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, val_check_interval, log_save_interval, row_log_interval, distributed_backend, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, amp_backend, amp_level, val_percent_check, test_percent_check, train_percent_check, overfit_pct)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_root_gpu_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36m_parse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPUs requested but none are available.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mYou\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mBut\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0monly\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mall_available_gpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \"\"\")\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: \n                You requested GPUs: [0]\n                But your machine only has: []\n            "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=6, logger=logger, gpus=[0], fast_dev_run=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Successful attack:103 Attack count:10000 Percentage:0.0103\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "98.97"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.adversarial_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}